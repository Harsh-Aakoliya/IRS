# -*- coding: utf-8 -*-
"""21BCE111_IRS_Practical_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uoU7YjIPkOepWIsXEjsw91QyIF-qhcGe

#### Problem Statement: Examine an Amazon product review dataset and implement text preprocessing methods, such as Tokenization, Stop word removal, Case folding, lemmatization/ Stemming, as well as part-of-speech tagging on the reviews. Extract feature and opinion pair from the review corpus and display review-wise feature and opinion pair from the data set
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# !ls

# !pip install kaggle
# !mkdir ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json
# !kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

# !ls

# !unzip imdb-dataset-of-50k-movie-reviews.zip

"""## Importing the dataset"""

data_path='/content/IMDB Dataset.csv'

df=pd.read_csv(data_path)

df

"""## Removing URL's, HTML Tags, and Puntuation"""

import re
import string
def remove_html_tags(text):
    pattern = re.compile('<.*?>')
    return pattern.sub(r'', text)

def remove_url(text):
    pattern = re.compile(r'https?://\S+ | www\. \S+')
    return pattern.sub(r'', text)


exclude = string.punctuation

def remove_punc(text):
    return text.translate(str.maketrans('', '', exclude))

def data_preprocess(df):
    df['review'] = df['review'].str.lower()
    df['review'] = df['review'].apply(remove_html_tags)
    df['review'] = df['review'].apply(remove_url)
    df['review'] = df['review'].apply(remove_punc)

data_preprocess(df)

"""## Result of the initial Pre-processing"""

df['review'][0]

import nltk
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

"""## Removing the stop Words"""

from nltk.corpus import stopwords

sw=set(stopwords.words('english'))
sw

def stopwords_removal(para):
    words=para.split()
    remword=[re for re in words if re not in sw]
    s=" ".join(remword)
    return s

df['review']=df['review'].apply(stopwords_removal)

df['review']

"""## Tokenizing the Dataset"""

from nltk.tokenize import word_tokenize

def tokenize(text):
    return word_tokenize(text)

df['review']=df['review'].apply(tokenize)

df

"""## Applying the Lemmatization on dataset"""

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

def lemmatize_words(word_list):
    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_list]
    return lemmatized_words

df['review'] = df['review'].apply(lemmatize_words)

df

"""## Appling the POS on the dataset"""

from nltk import pos_tag

nltk.download('averaged_perceptron_tagger')

def pos_words(word_list):
    part_of_speech = pos_tag(word_list)
    return part_of_speech

df['review'] = df['review'].apply(pos_words)

df

df.to_csv('./new_data.csv')

"""## Extracting the unique nouns form the dataset"""

unique_frequent_noun = {}

for i in df['review']:
    for j in range(len(i)):
        if i[j][1] == 'NN' and i[j][0] not in unique_frequent_noun:
            unique_frequent_noun[i[j][0]] = 1
        elif i[j][1] == 'NN' and i[j][0] in unique_frequent_noun:
            unique_frequent_noun[i[j][0]] += 1

sorted_unique_frequent_noun = dict(sorted(unique_frequent_noun.items(), key=lambda x: x[1], reverse=True))

"""## Extracting the top 20 frequent nouns"""

top_twenty_items = list(sorted_unique_frequent_noun.items())[:20]

top_twenty_items

"""## Creating the feature-opinion pair by taking +3 and -3 words near by frequent noun"""

def feature_opinion_pair(a):

  feature_opinion_dict = {}
  for i in range(len(a)):
      for j in top_twenty_items:
          if a[i][0] == j[0]:
              feature = a[i]
              context_window = a[max(0, i - 3): i + 4]
              opinion_words = [word for word in context_window if word != feature]

              feature_opinion_dict[feature] = opinion_words

  return feature_opinion_dict

df['feat_opi_pair']=df['review'].apply(feature_opinion_pair)

df

"""## Review and it's corroponding feature-opinion pair"""

df.drop(['sentiment'],axis=1)

df.to_csv('final_data.csv')
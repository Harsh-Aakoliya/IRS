# -*- coding: utf-8 -*-
"""21BCE111_IRS_Practical_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16jzlcYhppKXUW1nncavyYgPWKa3j_EWH
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import random
import nltk
nltk.download('punkt')

class SkipgramDataset(Dataset):
    def __init__(self, data, word_to_index, window_size=2):
        self.data = data
        self.window_size = window_size
        self.word_to_index = word_to_index
        self.samples = self.generate_samples()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

    def generate_samples(self):
        samples = []
        for sentence in self.data:
            for i, target_word in enumerate(sentence):
                context = self.get_context(sentence, i)
                target_index = self.word_to_index[target_word]
                context_indices = [self.word_to_index[word] for word in context]
                samples.extend([(target_index, context_index) for context_index in context_indices])
        return samples

    def get_context(self, sentence, target_index):
        start = max(0, target_index - self.window_size)
        end = min(len(sentence), target_index + self.window_size + 1)
        context = sentence[start:target_index] + sentence[target_index+1:end]
        return context

class Skipgram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(Skipgram, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)

    def forward(self, target):
        embedded_target = self.embedding(target)
        output = self.linear(embedded_target)
        return output

# Input sentences
sentences = [
    "The cat dances gracefully under the moonlight",
    "A group of adventurous friends embark on a thrilling journey",
    "Delicious aromas waft from the kitchen as the chef prepares a gourmet feast",
    "A gentle breeze rustles through the leaves of the ancient oak tree",
    "The old bookstore is filled with the scent of yellowed pages and memories",
    "Laughter echoes through the park as children play in the afternoon sun"
]

# Tokenize sentences
tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in sentences]

# Print tokenized sentences
for i, sentence in enumerate(tokenized_sentences):
    print(f"Sentence {i+1}: {sentence}")

# Print data in the desired format
print("\nData in the desired format:")
for sentence in tokenized_sentences:
    print(sentence)


data = tokenized_sentences

word_to_index = {}
index_to_word = {}
for sentence in data:
    for word in sentence:
        if word not in word_to_index:
            index = len(word_to_index)
            word_to_index[word] = index
            index_to_word[index] = word
vocab_size = len(word_to_index)

word_to_index, index_to_word

def one_hot_encoding(word_index, vocab_size):
    one_hot = np.zeros(vocab_size)
    one_hot[word_index] = 1
    return one_hot

one_hot_data = []
for sentence in data:
    one_hot_sentence = [one_hot_encoding(word_to_index[word], vocab_size) for word in sentence]
    one_hot_data.append(one_hot_sentence)

embedding_dim = 100
learning_rate = 0.001
num_epochs = 10
batch_size = 32

# Initialize dataset and dataloader
dataset = SkipgramDataset(data, word_to_index)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Initialize model, loss function, and optimizer
model = Skipgram(vocab_size, embedding_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    total_loss = 0
    for target, context in dataloader:
        optimizer.zero_grad()
        output = model(target)
        loss = criterion(output, context)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}')

def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot_product / (norm1 * norm2)

def get_embedding(word):
    print(word)
    index = word_to_index[word]
    with torch.no_grad():
        word_tensor = torch.tensor([index])
        embedding = model.embedding(word_tensor).numpy()
    return embedding.flatten()

# Flatten the list of tokenized sentences
flattened_sentences = [word for sentence in tokenized_sentences for word in sentence]

# Pick random 5 words from the flattened list
test_words = random.sample(flattened_sentences, 5)

test_words

for word in test_words:
    embedding = get_embedding(word)
    similarities = []
    for other_word in word_to_index.keys():
        if other_word != word:
            other_embedding = get_embedding(other_word)
            # print(other_embedding)
            similarity = cosine_similarity(embedding, other_embedding)
            similarities.append((other_word, similarity))
    similarities.sort(key=lambda x: x[1], reverse=True)
    print(f"Most similar words to '{word}':")
    for sim_word, sim in similarities[:5]:
        print(f"{sim_word}: {sim}")
    print()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import numpy as np
from collections import defaultdict

class CBOW(nn.Module):
    def __init__(self, vocab_size, embedding_dim, context_size):
        super(CBOW, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear1 = nn.Linear(embedding_dim, 128)
        self.linear2 = nn.Linear(128, vocab_size)

    def forward(self, inputs):
        embeds = torch.mean(self.embeddings(inputs), dim=0).view((1, -1))
        out = torch.relu(self.linear1(embeds))
        out = self.linear2(out)
        log_probs = torch.log_softmax(out, dim=1)
        return log_probs

def prepare_data(sentences, window_size=2):
    data = []
    word_to_ix = {}
    ix_to_word = {}
    word_freq = defaultdict(int)

    for sentence in sentences:
        for word in sentence:
            word_freq[word] += 1
            if word not in word_to_ix:
                word_to_ix[word] = len(word_to_ix)
                ix_to_word[len(ix_to_word)] = word

    for sentence in sentences:
        for i, target in enumerate(sentence):
            context = [sentence[j] for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)) if j != i]
            data.append((context, target))

    return data, word_to_ix, ix_to_word

def train_cbow_model(context_pairs, word_to_ix, embedding_dim=100, num_epochs=10, learning_rate=0.001):
    vocab_size = len(word_to_ix)
    context_size = 2 # context window size

    model = CBOW(vocab_size, embedding_dim, context_size)
    loss_function = nn.NLLLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)

    for epoch in range(num_epochs):
        total_loss = 0
        for context, target in context_pairs:
            context_idxs = make_context_vector(context, word_to_ix)
            target_idx = torch.tensor([word_to_ix[target]], dtype=torch.long)

            model.zero_grad()

            log_probs = model(context_idxs)
            loss = loss_function(log_probs, target_idx)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()
        print("Epoch {}, Loss: {:.4f}".format(epoch+1, total_loss))

def make_context_vector(context, word_to_ix):
    idxs = [word_to_ix[w] for w in context]
    return torch.tensor(idxs, dtype=torch.long)

# Example dataset
sentences = [
    ["I", "like", "playing", "football", "with", "friends"],
    ["She", "enjoys", "reading", "books"],
    ["We", "love", "eating", "pizza", "together"],
    ["He", "runs", "in", "the", "park"],
    ["They", "watch", "movies", "on", "weekends"]
]

# Prepare data and vocabulary
context_pairs, word_to_ix, ix_to_word = prepare_data(sentences)

# Train CBOW model
train_cbow_model(context_pairs, word_to_ix)
print(prepare_data(sentences,2))
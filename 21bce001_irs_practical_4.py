# -*- coding: utf-8 -*-
"""21BCE111_IRS_Practical_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fwfiz-GH65GZFmxsrNQG-hqssQ8U8ZGh

#### Problem Statement: Consider a corpus of N documents. Implement Vector Space Model Your implemented vector space model should rank the relevant retrieved documents by processing query.
"""

import numpy as np
import pandas as pd

"""## Importing the dataset"""

df=pd.read_csv('./dataset.csv')

df

df['Content'][0]

"""## Removing URL's, HTML Tags, and Puntuation"""

import re
import string

def remove_html_tags(text):
    pattern = re.compile('<.*?>')
    return pattern.sub(r'', text)

def remove_slash_n(text):
    pattern = re.compile('\n')
    return pattern.sub(r' ', text)

def remove_slash_t(text):
    pattern = re.compile('\t')
    return pattern.sub(r' ', text)

def remove_email(text):
    pattern = re.compile(r'\S+@\S+')
    return pattern.sub(r' ', text)

def remove_url(text):
    pattern = re.compile(r'https?://\S+ | www\. \S+')
    return pattern.sub(r'', text)

exclude = string.punctuation

def remove_punct(text):
    return text.translate(str.maketrans('', '', exclude))

def data_preprocess(df):
    df['Content'] = df['Content'].str.lower()
    df['Content'] = df['Content'].apply(remove_html_tags)
    df['Content'] = df['Content'].apply(remove_url)
    df['Content'] = df['Content'].apply(remove_email)
    df['Content'] = df['Content'].apply(remove_punct)
    df['Content'] = df['Content'].apply(remove_slash_n)
    df['Content'] = df['Content'].apply(remove_slash_t)
    # df['Content'] = df['Content'].apply(remove_extra_space)

data_preprocess(df)

df

df['Content'][0]

def remove_extra_space(text):
    pattern = ' '.join(text.split())
    return pattern

df['Content']=df['Content'].apply(remove_extra_space)

df['Content'][0]

import nltk
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

"""## Removing the stop Words"""

from nltk.corpus import stopwords

sw=set(stopwords.words('english'))

def stopwords_removal(para):
    words=para.split()
    remword=[re for re in words if re not in sw]
    s=" ".join(remword)
    return s

df['Content']=df['Content'].apply(stopwords_removal)

df['Content']

"""## Tokenizing the Dataset"""

from nltk.tokenize import word_tokenize

def tokenize(text):
    return word_tokenize(text)

df['Content']=df['Content'].apply(tokenize)

"""## Applying the Lemmatization on dataset"""

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

def lemmatize_words(word_list):
    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_list]
    return lemmatized_words

df['Content'] = df['Content'].apply(lemmatize_words)

df

"""## Getting Unique Words"""

unique_words=[]
for i in df['Content']:
    for j in i:
        if j not in unique_words:
            unique_words.append(j)

len(unique_words)

print(unique_words)

df

list_unique=np.zeros((len(unique_words),1000))
list_unique.shape

"""## Making N * M matrix, where M is the number of unique words and N is the number of documents"""

for i in range(len(unique_words)):
    for j in range(len(df['Content'])):
        count=0
        for k in df['Content'][j]:
            if unique_words[i] == k:
                count+=1
        list_unique[i][j]=(count)

list_unique

import pandas as pd
word_frequency_df = pd.DataFrame(list_unique, index=unique_words, columns=df.index)

word_frequency_df

word_frequency_df.to_csv("./word_frequency.csv")

"""## Inputting a query"""

query = 'article henry spencer writes in article paul johnson writes this bit interests me how much automatic control is there is it purely autonomous or is there some degree of ground control the stickandrudder man is always the onboard computer the computer normally gets its orders from a stored program but they can be overridden from the ground how is the transition from aerodynamic flight if thats what it is to hover accomplished this is the really new part its also one of the tricky parts there are four different ideas and dcx will probably end up trying all of them this is from talking to mitch burnside clapp whos one of the dcx test pilots at making orbit 1 pop a drogue chute from the nose light the engines once the thing stabilizes basefirst simple and reliable heavy shock loads on an area of structure that doesnt otherwise carry major loads needs a door in the hot part of the structure a door whose operation is missioncritical 2 switch off pitch stability the dc is aerodynamically unstable at subsonic speeds wait for it to flip and catch it at 180 degrees then light engines a bit scary 3 light the engines and use thrust vectoring to push the tail around probably the preferred method in the long run tricky because of the fuelfeed plumbing the fuel will start off in the tops of the tanks then slop down to the bottoms during the flip keeping the engines properly fed will be complicated 4 build up speed in a dive then pull up hard losing a lot of speed this things ld is not that great until its headed up and the vertical velocity drops to zero at which point it starts to fall tailfirst light engines also a bit scary and you probably dont have enough altitude left to try again all work is one mans work henry spencer u of toronto zoology kipling utzoohenry since the dcx is to take off horizontal why not land that way why do the martian landing thing or am i missing something dont know to much about dcx and such overly obvious why not just fall to earth like the russian crafts parachute in then michael adams im not high just jacked please enlighten me ignorance is easy to correct make a mistake and everyone will let you know you messed up'

"""## Query Pre-processing"""

words=query.split()

words

query_vec=[]
for i in unique_words:
    if i in words:
        query_vec.append(1)
    else:
        query_vec.append(0)

query_vec=np.array(query_vec)

"""## Calculating Cosine Similarity"""

cosine_similarities={}
count=0
for i in list_unique.T:
    a=np.dot(query_vec,i)/(np.linalg.norm(query_vec)*np.linalg.norm(i))
    cosine_similarities[count]=a
    count+=1

cosine_similarities

sorted_cosine_similarities= dict(sorted(cosine_similarities.items(), key=lambda item: item[1], reverse=True))

"""## Ranked Document Retrived Based on Similarity"""

sorted_cosine_similarities